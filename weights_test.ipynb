{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pyexpat import model\n",
    "import os\n",
    "import argparse\n",
    "from scipy.stats import spearmanr,pearsonr,kendalltau\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# /nfsdata/laip/results/valid/Llama-3___1-Tulu-3-8B_logits.json\n",
    "# /nfsdata/laip/results/valid/Mistral-7B-Instruct-v0___3_logits.json\n",
    "# /nfsdata/laip/results/valid/Meta-Llama-3___1-8B-Instruct_logits.json\n",
    "# /nfsdata/laip/results/valid/internlm3-8b-instruct_logits.json\n",
    "\n",
    "# /home/laip/InternalScore/results/valid/Llama-3___1-Tulu-3-8B_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/Mistral-7B-Instruct-v0___3_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/Meta-Llama-3___1-8B-Instruct_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/internlm3-8b-instruct_with_feedback_logits.json\n",
    "\n",
    "\n",
    "data_path = '/nfsdata/laip/results/valid/Meta-Llama-3___1-8B-Instruct_logits.json'\n",
    "data = json.load(open(data_path))\n",
    "# with open(data_path) as f:\n",
    "#     data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "import random\n",
    "random.seed(56)\n",
    "\n",
    "def optimize_layer_weights(logits_list, targets, loss_fn, num_epochs=2, lr=0.01,min_lr = 1e-3):\n",
    "    all_res=  []\n",
    "    L = len(logits_list[0])  \n",
    "    random.shuffle(logits_list)\n",
    "    \n",
    "    # 初始化权重\n",
    "    weights = torch.nn.Parameter(torch.ones(L, requires_grad=True))\n",
    "    optimizer = optim.Adam([weights], lr=lr)\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for sample_idx in trange(len(logits_list)):  # 遍历所有样本\n",
    "            logits = logits_list[sample_idx]  \n",
    "            target = targets[sample_idx]  \n",
    "\n",
    "            if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "                target = target - 1\n",
    "                target = torch.tensor(target,dtype=torch.long)\n",
    "\n",
    "            normalized_weights = torch.softmax(weights, dim=0)\n",
    "\n",
    "            # 计算加权和\n",
    "            weighted_sum = torch.zeros_like(logits[0])  \n",
    "            for l in range(L):\n",
    "                if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "                    weighted_sum += normalized_weights[l] * logits[l]  # logits累积加权\n",
    "                    predictions = weighted_sum\n",
    "                else:\n",
    "                    weighted_sum += normalized_weights[l] * (torch.tensor(logits[l])*torch.tensor([1,2,3,4,5])).sum()  # 加权求和\n",
    "                    predictions = weighted_sum  # 预测结果\n",
    "\n",
    "\n",
    "            loss = loss_fn(predictions,target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_res.append(total_loss/(sample_idx+1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        # 每个 epoch 打印损失\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return torch.softmax(weights, dim=0).detach(),all_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import json\n",
    "# from tqdm import tqdm,trange\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# all_loss = []\n",
    "# def optimize_layer_weights(logits_list, targets, loss_fn1,loss_fn2, num_epochs=4, lr=0.01 , add_loss = True):\n",
    "\n",
    "#     L = len(logits_list[0])  \n",
    "\n",
    "#     # 初始化权重\n",
    "#     weights = torch.nn.Parameter(torch.randn(L, requires_grad=True))\n",
    "#     beta = torch.nn.Parameter(torch.tensor(1, requires_grad=True,dtype=torch.float32))\n",
    "#     alpha = torch.nn.Parameter(torch.tensor(0, requires_grad=True,dtype=torch.float32))\n",
    "#     bias = torch.nn.Parameter(torch.tensor(0, requires_grad=True,dtype=torch.float32))\n",
    "#     # weights = torch.zeros(L)\n",
    "#     # weights[-1] = 1\n",
    "#     # weights = torch.nn.Parameter(weights,requires_grad=True)\n",
    "\n",
    "#     optimizer = optim.Adam([weights,beta,alpha,bias], lr=lr)\n",
    "\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         total_loss = 0\n",
    "\n",
    "#         for sample_idx in trange(len(logits_list)):  # 遍历所有样本\n",
    "#             logits = logits_list[sample_idx]  \n",
    "#             target = targets[sample_idx]  \n",
    "\n",
    "#             target1 = target - 1\n",
    "#             target1 = torch.tensor(target1,dtype=torch.long)\n",
    "#             target2 = target\n",
    "\n",
    "#             normalized_weights = torch.softmax(weights, dim=0)\n",
    "\n",
    "#             # 计算加权和\n",
    "#             weighted_sum1 = torch.zeros_like(logits[0])  \n",
    "#             weighted_sum2 = torch.tensor(0,dtype = torch.float32)\n",
    "\n",
    "#             for l in range(L):\n",
    "                \n",
    "#                 weighted_sum1 += normalized_weights[l] * logits[l]  # logits累积加权\n",
    "#                 weighted_sum2 += (normalized_weights[l] * (torch.tensor(logits[l]).softmax(dim=-1)*torch.tensor([1,2,3,4,5])).sum()).sum()  # 加权求和\n",
    "                \n",
    "#             predictions1 = weighted_sum1\n",
    "#             predictions2 = beta*weighted_sum2.sum()+bias  # 预测结果\n",
    "\n",
    "#             alpha_norm = torch.sigmoid(alpha)\n",
    "#             loss = loss_fn1(predictions1,target1) +alpha_norm*loss_fn2(predictions2,target2)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             all_loss.append(total_loss/(sample_idx+1))\n",
    "\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         avg_loss = total_loss / len(logits_list)\n",
    "#         # 每个 epoch 打印损失\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     return torch.softmax(weights, dim=0).detach(),beta.detach(),bias.detach(),all_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "def optimize_layer_weights1(\n",
    "    logits_list,\n",
    "    targets,\n",
    "    loss_fn1,\n",
    "    loss_fn2,\n",
    "    num_epochs=4,\n",
    "    batch_size=32,\n",
    "    lr=0.02,\n",
    "    min_lr=1e-4,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"带余弦退火学习率调度和批量训练的优化版本\"\"\"\n",
    "    # 数据预处理 -------------------------------------------------\n",
    "    # 转换为张量并确保设备一致性\n",
    "    L = len(logits_list[0])\n",
    "    num_classes = len(logits_list[0][0])\n",
    "    \n",
    "    # 将输入数据转换为张量 [num_samples, L, num_classes]\n",
    "    logits_tensor = torch.tensor(logits_list,device=device)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "    \n",
    "    # 转换目标张量\n",
    "    \n",
    "    targets_reg = torch.tensor(targets, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 系数张量\n",
    "    coefficients = torch.tensor([1.,2.,3.,4.,5.], device=device)\n",
    "    \n",
    "    # 参数初始化 -------------------------------------------------\n",
    "    weights = nn.Parameter(torch.ones(L, device=device))\n",
    "    beta = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    alpha = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    bias = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "    \n",
    "    # 优化器和调度器 ---------------------------------------------\n",
    "    #optimizer = optim.Adam([weights, beta, alpha, bias], lr=lr)\n",
    "    optimizer = optim.Adam([weights], lr=lr)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=min_lr)\n",
    "    \n",
    "    # 训练循环 ---------------------------------------------------\n",
    "    all_losses = []\n",
    "    num_samples = len(logits_list)\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        # 随机打乱数据\n",
    "        indices = torch.randperm(num_samples)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_start in range(0, num_samples, batch_size):\n",
    "            # 获取当前batch\n",
    "            batch_indices = indices[batch_start : batch_start+batch_size]\n",
    "            batch_logits = logits_tensor[batch_indices]  # [B, L, C]\n",
    "            batch_cls_targets = targets[batch_indices]-1\n",
    "            batch_reg_targets = targets_reg[batch_indices]\n",
    "            \n",
    "            # 前向传播 -------------------------------------------\n",
    "            normalized_weights = torch.softmax(weights, dim=0)\n",
    "            \n",
    "            # 分类预测 [B, C]\n",
    "            cls_pred = torch.einsum('l,blc->bc', normalized_weights, batch_logits)\n",
    "            \n",
    "            # 回归预测计算\n",
    "            # 1. 计算每个样本每层的得分 [B, L]\n",
    "            layer_scores = torch.softmax(batch_logits, dim=-1) @ coefficients\n",
    "            # 2. 加权求和 [B]\n",
    "            reg_pred = beta * (layer_scores @ normalized_weights) + bias\n",
    "            \n",
    "            # 损失计算 -------------------------------------------\n",
    "            alpha_norm = torch.sigmoid(alpha)\n",
    "            loss_cls = loss_fn1(cls_pred, batch_cls_targets)\n",
    "            loss_reg = loss_fn2(reg_pred, batch_reg_targets)\n",
    "            #total_loss = alpha_norm * loss_cls + (1 - alpha_norm) * loss_reg\n",
    "            total_loss = loss_cls\n",
    "            \n",
    "            # 反向传播 -------------------------------------------\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录损失\n",
    "            all_losses.append(total_loss.item())\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 打印统计信息\n",
    "        avg_loss = epoch_loss / (num_samples // batch_size + 1)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "    return (\n",
    "        torch.softmax(weights, dim=0).detach(),\n",
    "        beta.detach(),\n",
    "        bias.detach(),\n",
    "        all_losses\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_score_ls = []\n",
    "logits_ls = []\n",
    "human_score_ls1 = []\n",
    "\n",
    "weighted_score_ls = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "for data1 in data:\n",
    "    df = pd.DataFrame(data1['df'])\n",
    "    if data1['weighted_socre']==-1:\n",
    "        continue\n",
    "    _logits = torch.tensor([i for i in df['logits']],dtype=torch.float32)\n",
    "    #_logits = [i for i in df['logits']]\n",
    "    human_score_ls1.append(data1['human_score'])\n",
    "    if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "        human_score = torch.tensor(data1['human_score'],dtype=torch.long)\n",
    "        \n",
    "    else:\n",
    "        human_score = torch.tensor(data1['human_score'],dtype=torch.float32)\n",
    "\n",
    "    weighted_score = torch.tensor(df['weighted_score'].to_list(),dtype=torch.float32)\n",
    "    human_score_ls.append(human_score)\n",
    "    logits_ls.append(_logits)\n",
    "    weighted_score_ls.append(weighted_score)\n",
    "# for data1 in data:\n",
    "#     df = pd.DataFrame(data1['res'])\n",
    "#     if data1['pred_score']['weighted_socre']==-1:\n",
    "#         continue\n",
    "#     _logits = torch.tensor([i for i in df['logits']],dtype=torch.float32)\n",
    "\n",
    "#     if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "#         human_score = torch.tensor(int(data1['human']),dtype=torch.long)\n",
    "#     else:\n",
    "#         human_score = torch.tensor(int(data1['human']),dtype=torch.float32)\n",
    "\n",
    "#     weighted_score = torch.tensor(df['weighted_score'].to_list(),dtype=torch.float32)\n",
    "#     human_score_ls.append(human_score)\n",
    "#     logits_ls.append(_logits)\n",
    "#     weighted_score_ls.append(weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:01<00:00, 759.22it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 1604.9973\n",
      "学习到的权重： tensor([0.0517, 0.0506, 0.0500, 0.0540, 0.0480, 0.0429, 0.0282, 0.0408, 0.0402,\n",
      "        0.0423, 0.0540, 0.0499, 0.0479, 0.0590, 0.0576, 0.0458, 0.0537, 0.0324,\n",
      "        0.0187, 0.0210, 0.0218, 0.0157, 0.0135, 0.0188, 0.0053, 0.0045, 0.0044,\n",
      "        0.0053, 0.0043, 0.0043, 0.0041, 0.0039, 0.0052])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2688244e90>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights,beta,bias,all_loss = optimize_layer_weights(\n",
    "#                                         logits_ls, \n",
    "#                                         torch.tensor(human_score_ls1), \n",
    "#                                         loss_fn1 = nn.CrossEntropyLoss(),\n",
    "#                                         loss_fn2=nn.MSELoss(),\n",
    "#                                         num_epochs=10, \n",
    "#                                         batch_size = 128,\n",
    "#                                         lr=0.0001,\n",
    "#                                         min_lr=1e-4,\n",
    "#                                         )\n",
    "weights,all_loss = optimize_layer_weights(logits_ls, human_score_ls, nn.CrossEntropyLoss(),num_epochs=1, lr=0.01)\n",
    "\n",
    "print(\"学习到的权重：\", weights)\n",
    "weights = weights.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05170907, 0.05060843, 0.05004781, 0.05398959, 0.04801609,\n",
       "       0.04292724, 0.02820304, 0.04084966, 0.04022837, 0.04226979,\n",
       "       0.05401829, 0.04989972, 0.04789615, 0.05896118, 0.05761898,\n",
       "       0.04576585, 0.05368108, 0.03242693, 0.01874143, 0.02099193,\n",
       "       0.02184623, 0.01572761, 0.01349316, 0.01882451, 0.00527081,\n",
       "       0.00449068, 0.00435522, 0.00530916, 0.00433462, 0.0043277 ,\n",
       "       0.00408142, 0.00392742, 0.00516077], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/laip/InternalScore/results/flask/Meta-Llama-3___1-8B-Instruct_logits.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#data_path = '/home/laip/InternalScore/results/flask/Llama-2-7b-chat-hf_logits.json'\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#data_path = '/home/laip/InternalScore/results/flask/Mistral-7B-Instruct-v0___2_logits.json'\u001b[39;00m\n\u001b[1;32m     44\u001b[0m all_human_score \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m all_res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(data_path))\n\u001b[1;32m     47\u001b[0m direct_score_ls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m weighted_score_ls \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/laip/InternalScore/results/flask/Meta-Llama-3___1-8B-Instruct_logits.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_corr(pred_score, score_type,human_score,type_r = 'pearson' ):\n",
    "    r_ls = ['pearson','spearman','kendalltau']\n",
    "    with open(f'predscore_{score_type}.json','w') as f:\n",
    "        json.dump(pred_score,f)\n",
    "    if type_r not in r_ls:\n",
    "        raise ValueError('type_r must be one of {}'.format(r_ls))\n",
    "    elif type_r == 'pearson':\n",
    "        r = pearsonr(pred_score, human_score)[0]\n",
    "    elif type_r == 'kendalltau':\n",
    "        r = kendalltau(pred_score, human_score)[0]\n",
    "    else:\n",
    "        r = spearmanr(pred_score, human_score)[0]\n",
    "    return r\n",
    "\n",
    "def print_correlations(all_score_dict,human_score):\n",
    "    metrics = ['pearson','spearman','kendalltau']   \n",
    "    scores = ['direct_score','weighted_score','weighted_direct_score','internalscore','pre_score1','pre_score3','pre_score2','pre_score4']\n",
    "    table = PrettyTable(['score_type']+metrics)\n",
    "    for score in scores:\n",
    "        add_row = [score] +[round(calc_corr(all_score_dict[score],score_type=score,human_score=human_score,type_r = 'pearson'),3),\n",
    "                            round(calc_corr(all_score_dict[score],score_type='',human_score=human_score,type_r = 'spearman'),3),\n",
    "                            round(calc_corr(all_score_dict[score],score_type='',human_score=human_score,type_r = 'kendalltau'),3)]\n",
    "        table.add_row(add_row)\n",
    "    print(table)\n",
    "\n",
    "if 'tulu' in data_path.lower():\n",
    "    model_name = 'Llama-3___1-Tulu-3-8B'\n",
    "elif 'llama' in data_path.lower():\n",
    "    model_name = 'Meta-Llama-3___1-8B-Instruct'\n",
    "elif 'mistral' in data_path.lower():\n",
    "    model_name = 'Mistral-7B-Instruct-v0___3'\n",
    "elif 'internlm' in data_path.lower():\n",
    "    model_name = 'internlm3-8b-instruct'\n",
    "elif 'qwen' in data_path.lower():\n",
    "    model_name = 'Qwen2___5-7B-Instruct'\n",
    "\n",
    "#data_path = f'/home/laip/InternalScore/results/helpsteer/{model_name}_with_feedback_logits.json'\n",
    "#data_path = f'/home/laip/InternalScore/results/helpsteer/{model_name}_logits.json'\n",
    "\n",
    "#data_path = f'/home/laip/InternalScore/results/flask/{model_name}_with_feedback_logits.json'\n",
    "data_path = f'/home/laip/InternalScore/results/flask/{model_name}_logits.json'\n",
    "#data_path = '/home/laip/InternalScore/results/flask/Llama-2-7b-chat-hf_logits.json'\n",
    "#data_path = '/home/laip/InternalScore/results/flask/Mistral-7B-Instruct-v0___2_logits.json'\n",
    "all_human_score = []\n",
    "\n",
    "all_res = json.load(open(data_path))\n",
    "direct_score_ls = []\n",
    "weighted_score_ls = []\n",
    "weighted_direct_score_ls= []\n",
    "internalscore_ls = []\n",
    "save_ls = []\n",
    "pre_score_ls1 = []\n",
    "pre_score_ls2 = []\n",
    "pre_score_ls3 = []\n",
    "pre_score_ls4 = []\n",
    "ratio = 0.9\n",
    "cos = 0.02\n",
    "score = np.array([[1,2,3,4,5] for i in range(pd.DataFrame(all_res[0]['df']).shape[0])])\n",
    "final_score  = pd.DataFrame(score)\n",
    "\n",
    "\n",
    "for i in range(len(all_res)):\n",
    "    res = all_res[i]\n",
    "    \n",
    "    if res['weighted_socre'] == -1:\n",
    "        continue\n",
    "    all_human_score.append(res['human_score'])\n",
    "    df = pd.DataFrame(res['df']) \n",
    "    score = np.array([1,2,3,4,5])\n",
    "\n",
    "    # weighed_score 加权\n",
    "    distribution1 = df['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32).softmax(dim=-1))\n",
    "    pre_score1 = ((distribution1.apply(lambda x:(x*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum()))*weights).sum().item()\n",
    "\n",
    "    # 累积logits 加权\n",
    "    logits = df['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    distribution2 = (logits).apply(lambda x:torch.tensor(x,dtype=torch.float32).argmax(dim=-1))\n",
    "    pre_score = ((torch.tensor([ i+1 for i in distribution2]))*weights).sum().item()\n",
    "\n",
    "    distribution2 = torch.softmax((logits*weights).sum(),dim=-1)\n",
    "    pre_score2 = ((distribution2*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum()).item()\n",
    "    #print(pre_score2)\n",
    "    #pre_score2 = torch.argmax(distribution2).item()+1\n",
    "\n",
    "    # 累积logits 不加权    \n",
    "    distribution3 = torch.softmax((logits/weights.shape[0]).sum(),dim=-1)\n",
    "    pre_score3 = (distribution3*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum().item()\n",
    "\n",
    "    # ratio + cosine similarity 筛选\n",
    "   # logits = df[(df['ratio']>ratio)&(df['weights']>cos)]['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    logits = df[(df['ratio']>ratio)]['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    distribution4 = torch.softmax((logits/logits.shape[0]).sum(),dim=-1)\n",
    "    pre_score4 = (distribution4*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum().item()\n",
    "\n",
    "    prompt = res['prompt']\n",
    "    direct_score_ls.append(res['direct_socre'])\n",
    "    weighted_score_ls.append(res['weighted_socre'])\n",
    "    weighted_direct_score_ls.append(res['weighted_direct_socre'])\n",
    "    internalscore_ls.append(res['internalscore'])\n",
    "    pre_score_ls1.append(pre_score1)\n",
    "    pre_score_ls2.append(pre_score2)\n",
    "    pre_score_ls3.append(pre_score3)\n",
    "    pre_score_ls4.append(pre_score)\n",
    "    \n",
    "    #internalscore_ls.append((df['weighted_score']*weights).sum())\n",
    "#print(df[df['ratio']>r].shape[0])\n",
    "print(len(all_human_score),len(direct_score_ls),len(weighted_score_ls),len(weighted_direct_score_ls),len(internalscore_ls),len(pre_score_ls1))\n",
    "all_score_dict = {'direct_score':direct_score_ls,\n",
    "                  'weighted_score':weighted_score_ls,\n",
    "                  'weighted_direct_score':weighted_direct_score_ls,\n",
    "                  'internalscore':internalscore_ls,\n",
    "                  'pre_score1':pre_score_ls1,\n",
    "                  'pre_score2':pre_score_ls2,\n",
    "                  'pre_score3':pre_score_ls3,\n",
    "                  'pre_score4':pre_score_ls4\n",
    "                  }\n",
    "\n",
    "\n",
    "print_correlations(all_score_dict,all_human_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.064607928902617"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_abs(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return (np.abs(x-y)/len(x)).sum()\n",
    "cal_abs(pre_score_ls2,all_human_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_score_ls \n",
    "all_human_score\n",
    "pre_score_ls2\n",
    "pre_score_ls3\n",
    "data_name1 = 'direct_score'\n",
    "with open(f'{data_name1}.json','w') as f:\n",
    "    json.dump(direct_score_ls,f)\n",
    "data_name2 = 'human_score'\n",
    "with open(f'{data_name2}.json','w') as f:\n",
    "    json.dump(all_human_score,f)\n",
    "data_name3 = 'palmscore_w_tuning'\n",
    "with open(f'{data_name3}.json','w') as f:\n",
    "    json.dump(pre_score_ls3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "for i in range(1,7):\n",
    "    for j in range(1,7):\n",
    "        if i**2==4*j:\n",
    "            cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n",
    "\trow_len,col_len = len(a),len(a[0])\n",
    "\tb = [[0]*row_len]*col_len\n",
    "\tfor i in range(row_len):\n",
    "\t\tfor j in range(col_len):\n",
    "\t\t\tb[j][i] = a[i][j]\n",
    "\t\tprint(b)\n",
    "\treturn b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n",
    "\t#Write your code here and return a python list after reshaping by using numpy's tolist() method\n",
    "\trow_len,col_len = new_shape[0],new_shape[1]\n",
    "\tall_data = []\n",
    "\tfor i in range(len(a)):\n",
    "\t\tfor j in range(len(a[0])):\n",
    "\t\t\tall_data.append(a[i][j])\n",
    "\treshaped_matrix = [[0]*col_len for _ in range(row_len)]\n",
    "\tfor i in range(row_len):\n",
    "\t\tfor j in range(col_len):\n",
    "\t\t\treshaped_matrix[i][j] = all_data[i*col_len+j]\n",
    "\treturn reshaped_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3132616875182228"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
