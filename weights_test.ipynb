{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pyexpat import model\n",
    "import os\n",
    "import argparse\n",
    "from scipy.stats import spearmanr,pearsonr,kendalltau\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# /nfsdata/laip/results/valid/Llama-3___1-Tulu-3-8B_logits.json\n",
    "# /nfsdata/laip/results/valid/Mistral-7B-Instruct-v0___3_logits.json\n",
    "# /nfsdata/laip/results/valid/Meta-Llama-3___1-8B-Instruct_logits.json\n",
    "# /nfsdata/laip/results/valid/internlm3-8b-instruct_logits.json\n",
    "\n",
    "# /home/laip/InternalScore/results/valid/Llama-3___1-Tulu-3-8B_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/Mistral-7B-Instruct-v0___3_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/Meta-Llama-3___1-8B-Instruct_with_feedback_logits.json\n",
    "# /home/laip/InternalScore/results/valid/internlm3-8b-instruct_with_feedback_logits.json\n",
    "\n",
    "\n",
    "data_path = '/nfsdata/laip/results/valid/Meta-Llama-3___1-8B-Instruct_logits.json'\n",
    "data = json.load(open(data_path))\n",
    "# with open(data_path) as f:\n",
    "#     data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "\n",
    "def optimize_layer_weights(logits_list, targets, loss_fn, num_epochs=2, lr=0.01,min_lr = 1e-3):\n",
    "    all_res=  []\n",
    "    L = len(logits_list[0])  \n",
    "    \n",
    "    # 初始化权重\n",
    "    weights = torch.nn.Parameter(torch.ones(L, requires_grad=True))\n",
    "    optimizer = optim.Adam([weights], lr=lr)\n",
    "\n",
    "    for epoch in trange(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for sample_idx in trange(len(logits_list)):  # 遍历所有样本\n",
    "            logits = logits_list[sample_idx]  \n",
    "            target = targets[sample_idx]  \n",
    "\n",
    "            if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "                target = target - 1\n",
    "                target = torch.tensor(target,dtype=torch.long)\n",
    "\n",
    "            normalized_weights = torch.softmax(weights, dim=0)\n",
    "\n",
    "            # 计算加权和\n",
    "            weighted_sum = torch.zeros_like(logits[0])  \n",
    "            for l in range(L):\n",
    "                if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "                    weighted_sum += normalized_weights[l] * logits[l]  # logits累积加权\n",
    "                    predictions = weighted_sum\n",
    "                else:\n",
    "                    weighted_sum += normalized_weights[l] * (torch.tensor(logits[l])*torch.tensor([1,2,3,4,5])).sum()  # 加权求和\n",
    "                    predictions = weighted_sum  # 预测结果\n",
    "\n",
    "\n",
    "            loss = loss_fn(predictions,target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_res.append(total_loss/(sample_idx+1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        # 每个 epoch 打印损失\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return torch.softmax(weights, dim=0).detach(),all_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import json\n",
    "# from tqdm import tqdm,trange\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# all_loss = []\n",
    "# def optimize_layer_weights(logits_list, targets, loss_fn1,loss_fn2, num_epochs=4, lr=0.01 , add_loss = True):\n",
    "\n",
    "#     L = len(logits_list[0])  \n",
    "\n",
    "#     # 初始化权重\n",
    "#     weights = torch.nn.Parameter(torch.randn(L, requires_grad=True))\n",
    "#     beta = torch.nn.Parameter(torch.tensor(1, requires_grad=True,dtype=torch.float32))\n",
    "#     alpha = torch.nn.Parameter(torch.tensor(0, requires_grad=True,dtype=torch.float32))\n",
    "#     bias = torch.nn.Parameter(torch.tensor(0, requires_grad=True,dtype=torch.float32))\n",
    "#     # weights = torch.zeros(L)\n",
    "#     # weights[-1] = 1\n",
    "#     # weights = torch.nn.Parameter(weights,requires_grad=True)\n",
    "\n",
    "#     optimizer = optim.Adam([weights,beta,alpha,bias], lr=lr)\n",
    "\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         total_loss = 0\n",
    "\n",
    "#         for sample_idx in trange(len(logits_list)):  # 遍历所有样本\n",
    "#             logits = logits_list[sample_idx]  \n",
    "#             target = targets[sample_idx]  \n",
    "\n",
    "#             target1 = target - 1\n",
    "#             target1 = torch.tensor(target1,dtype=torch.long)\n",
    "#             target2 = target\n",
    "\n",
    "#             normalized_weights = torch.softmax(weights, dim=0)\n",
    "\n",
    "#             # 计算加权和\n",
    "#             weighted_sum1 = torch.zeros_like(logits[0])  \n",
    "#             weighted_sum2 = torch.tensor(0,dtype = torch.float32)\n",
    "\n",
    "#             for l in range(L):\n",
    "                \n",
    "#                 weighted_sum1 += normalized_weights[l] * logits[l]  # logits累积加权\n",
    "#                 weighted_sum2 += (normalized_weights[l] * (torch.tensor(logits[l]).softmax(dim=-1)*torch.tensor([1,2,3,4,5])).sum()).sum()  # 加权求和\n",
    "                \n",
    "#             predictions1 = weighted_sum1\n",
    "#             predictions2 = beta*weighted_sum2.sum()+bias  # 预测结果\n",
    "\n",
    "#             alpha_norm = torch.sigmoid(alpha)\n",
    "#             loss = loss_fn1(predictions1,target1) +alpha_norm*loss_fn2(predictions2,target2)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             all_loss.append(total_loss/(sample_idx+1))\n",
    "\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         avg_loss = total_loss / len(logits_list)\n",
    "#         # 每个 epoch 打印损失\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     return torch.softmax(weights, dim=0).detach(),beta.detach(),bias.detach(),all_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "def optimize_layer_weights(\n",
    "    logits_list,\n",
    "    targets,\n",
    "    loss_fn1,\n",
    "    loss_fn2,\n",
    "    num_epochs=4,\n",
    "    batch_size=32,\n",
    "    lr=0.02,\n",
    "    min_lr=1e-4,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"带余弦退火学习率调度和批量训练的优化版本\"\"\"\n",
    "    # 数据预处理 -------------------------------------------------\n",
    "    # 转换为张量并确保设备一致性\n",
    "    L = len(logits_list[0])\n",
    "    num_classes = len(logits_list[0][0])\n",
    "    \n",
    "    # 将输入数据转换为张量 [num_samples, L, num_classes]\n",
    "    logits_tensor = torch.tensor(logits_list,device=device)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "    \n",
    "    # 转换目标张量\n",
    "    \n",
    "    targets_reg = torch.tensor(targets, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 系数张量\n",
    "    coefficients = torch.tensor([1.,2.,3.,4.,5.], device=device)\n",
    "    \n",
    "    # 参数初始化 -------------------------------------------------\n",
    "    weights = nn.Parameter(torch.ones(L, device=device))\n",
    "    beta = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    alpha = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    bias = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "    \n",
    "    # 优化器和调度器 ---------------------------------------------\n",
    "    #optimizer = optim.Adam([weights, beta, alpha, bias], lr=lr)\n",
    "    optimizer = optim.Adam([weights], lr=lr)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=min_lr)\n",
    "    \n",
    "    # 训练循环 ---------------------------------------------------\n",
    "    all_losses = []\n",
    "    num_samples = len(logits_list)\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        # 随机打乱数据\n",
    "        indices = torch.randperm(num_samples)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_start in range(0, num_samples, batch_size):\n",
    "            # 获取当前batch\n",
    "            batch_indices = indices[batch_start : batch_start+batch_size]\n",
    "            batch_logits = logits_tensor[batch_indices]  # [B, L, C]\n",
    "            batch_cls_targets = targets[batch_indices]-1\n",
    "            batch_reg_targets = targets_reg[batch_indices]\n",
    "            \n",
    "            # 前向传播 -------------------------------------------\n",
    "            normalized_weights = torch.softmax(weights, dim=0)\n",
    "            \n",
    "            # 分类预测 [B, C]\n",
    "            cls_pred = torch.einsum('l,blc->bc', normalized_weights, batch_logits)\n",
    "            \n",
    "            # 回归预测计算\n",
    "            # 1. 计算每个样本每层的得分 [B, L]\n",
    "            layer_scores = torch.softmax(batch_logits, dim=-1) @ coefficients\n",
    "            # 2. 加权求和 [B]\n",
    "            reg_pred = beta * (layer_scores @ normalized_weights) + bias\n",
    "            \n",
    "            # 损失计算 -------------------------------------------\n",
    "            alpha_norm = torch.sigmoid(alpha)\n",
    "            loss_cls = loss_fn1(cls_pred, batch_cls_targets)\n",
    "            loss_reg = loss_fn2(reg_pred, batch_reg_targets)\n",
    "            #total_loss = alpha_norm * loss_cls + (1 - alpha_norm) * loss_reg\n",
    "            total_loss = loss_cls\n",
    "            \n",
    "            # 反向传播 -------------------------------------------\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录损失\n",
    "            all_losses.append(total_loss.item())\n",
    "            epoch_loss += total_loss.item()\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 打印统计信息\n",
    "        avg_loss = epoch_loss / (num_samples // batch_size + 1)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "    return (\n",
    "        torch.softmax(weights, dim=0).detach(),\n",
    "        beta.detach(),\n",
    "        bias.detach(),\n",
    "        all_losses\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_score_ls = []\n",
    "logits_ls = []\n",
    "human_score_ls1 = []\n",
    "\n",
    "weighted_score_ls = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.MSELoss()\n",
    "\n",
    "for data1 in data:\n",
    "    df = pd.DataFrame(data1['df'])\n",
    "    if data1['weighted_socre']==-1:\n",
    "        continue\n",
    "    _logits = torch.tensor([i for i in df['logits']],dtype=torch.float32)\n",
    "    #_logits = [i for i in df['logits']]\n",
    "    human_score_ls1.append(data1['human_score'])\n",
    "    if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "        human_score = torch.tensor(data1['human_score'],dtype=torch.long)\n",
    "        \n",
    "    else:\n",
    "        human_score = torch.tensor(data1['human_score'],dtype=torch.float32)\n",
    "\n",
    "    weighted_score = torch.tensor(df['weighted_score'].to_list(),dtype=torch.float32)\n",
    "    human_score_ls.append(human_score)\n",
    "    logits_ls.append(_logits)\n",
    "    weighted_score_ls.append(weighted_score)\n",
    "# for data1 in data:\n",
    "#     df = pd.DataFrame(data1['res'])\n",
    "#     if data1['pred_score']['weighted_socre']==-1:\n",
    "#         continue\n",
    "#     _logits = torch.tensor([i for i in df['logits']],dtype=torch.float32)\n",
    "\n",
    "#     if type(loss_fn) == torch.nn.modules.loss.CrossEntropyLoss:\n",
    "#         human_score = torch.tensor(int(data1['human']),dtype=torch.long)\n",
    "#     else:\n",
    "#         human_score = torch.tensor(int(data1['human']),dtype=torch.float32)\n",
    "\n",
    "#     weighted_score = torch.tensor(df['weighted_score'].to_list(),dtype=torch.float32)\n",
    "#     human_score_ls.append(human_score)\n",
    "#     logits_ls.append(_logits)\n",
    "#     weighted_score_ls.append(weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimize_layer_weights() missing 1 required positional argument: 'loss_fn2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# weights,beta,bias,all_loss = optimize_layer_weights(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#                                         logits_ls, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#                                         torch.tensor(human_score_ls1), \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#                                         min_lr=1e-4,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#                                         )\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m weights,all_loss \u001b[38;5;241m=\u001b[39m optimize_layer_weights(logits_ls, human_score_ls, nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m学习到的权重：\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights)\n\u001b[1;32m     14\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mTypeError\u001b[0m: optimize_layer_weights() missing 1 required positional argument: 'loss_fn2'"
     ]
    }
   ],
   "source": [
    "# weights,beta,bias,all_loss = optimize_layer_weights(\n",
    "#                                         logits_ls, \n",
    "#                                         torch.tensor(human_score_ls1), \n",
    "#                                         loss_fn1 = nn.CrossEntropyLoss(),\n",
    "#                                         loss_fn2=nn.MSELoss(),\n",
    "#                                         num_epochs=10, \n",
    "#                                         batch_size = 128,\n",
    "#                                         lr=0.0001,\n",
    "#                                         min_lr=1e-4,\n",
    "#                                         )\n",
    "weights,all_loss = optimize_layer_weights(logits_ls, human_score_ls, nn.CrossEntropyLoss(),num_epochs=1, lr=0.01)\n",
    "\n",
    "print(\"学习到的权重：\", weights)\n",
    "weights = weights.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00343718, 0.00346947, 0.00337097, 0.00337627, 0.00320193,\n",
       "       0.00302232, 0.00270045, 0.00313989, 0.00311932, 0.00321641,\n",
       "       0.0035409 , 0.00358735, 0.003945  , 0.00419683, 0.00361342,\n",
       "       0.00348207, 0.00357652, 0.00297285, 0.00281725, 0.00243641,\n",
       "       0.00243769, 0.00244864, 0.0036508 , 0.8593869 , 0.00316575,\n",
       "       0.00346953, 0.0035898 , 0.00938825, 0.00595825, 0.00517091,\n",
       "       0.00527246, 0.0053073 , 0.02453089], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/laip/InternalScore/results/flask/Meta-Llama-3___1-8B-Instruct_logits.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#data_path = '/home/laip/InternalScore/results/flask/Llama-2-7b-chat-hf_logits.json'\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#data_path = '/home/laip/InternalScore/results/flask/Mistral-7B-Instruct-v0___2_logits.json'\u001b[39;00m\n\u001b[1;32m     53\u001b[0m all_human_score \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 56\u001b[0m all_res \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(data_path))\n\u001b[1;32m     57\u001b[0m direct_score_ls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m weighted_score_ls \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/laip/InternalScore/results/flask/Meta-Llama-3___1-8B-Instruct_logits.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_corr(pred_score, score_type,human_score,type_r = 'pearson' ):\n",
    "    r_ls = ['pearson','spearman','kendalltau']\n",
    "    with open(f'predscore_{score_type}.json','w') as f:\n",
    "        json.dump(pred_score,f)\n",
    "    if type_r not in r_ls:\n",
    "        raise ValueError('type_r must be one of {}'.format(r_ls))\n",
    "    elif type_r == 'pearson':\n",
    "        r = pearsonr(pred_score, human_score)[0]\n",
    "    elif type_r == 'kendalltau':\n",
    "        r = kendalltau(pred_score, human_score)[0]\n",
    "    else:\n",
    "        r = spearmanr(pred_score, human_score)[0]\n",
    "    return r\n",
    "\n",
    "def print_correlations(all_score_dict,human_score):\n",
    "    metrics = ['pearson','spearman','kendalltau']   \n",
    "    scores = ['direct_score','weighted_score','weighted_direct_score','internalscore','pre_score1','pre_score3','pre_score2','pre_score4']\n",
    "    table = PrettyTable(['score_type']+metrics)\n",
    "    for score in scores:\n",
    "        add_row = [score] +[round(calc_corr(all_score_dict[score],score_type=score,human_score=human_score,type_r = 'pearson'),3),\n",
    "                            round(calc_corr(all_score_dict[score],score_type='',human_score=human_score,type_r = 'spearman'),3),\n",
    "                            round(calc_corr(all_score_dict[score],score_type='',human_score=human_score,type_r = 'kendalltau'),3)]\n",
    "        table.add_row(add_row)\n",
    "    print(table)\n",
    "\n",
    "if 'tulu' in data_path.lower():\n",
    "    model_name = 'Llama-3___1-Tulu-3-8B'\n",
    "elif 'llama' in data_path.lower():\n",
    "    model_name = 'Meta-Llama-3___1-8B-Instruct'\n",
    "elif 'mistral' in data_path.lower():\n",
    "    model_name = 'Mistral-7B-Instruct-v0___3'\n",
    "elif 'internlm' in data_path.lower():\n",
    "    model_name = 'internlm3-8b-instruct'\n",
    "elif 'qwen' in data_path.lower():\n",
    "    model_name = 'Qwen2___5-7B-Instruct'\n",
    "\n",
    "#data_path = f'/home/laip/InternalScore/results/helpsteer/{model_name}_with_feedback_logits.json'\n",
    "#data_path = f'/home/laip/InternalScore/results/helpsteer/{model_name}_logits.json'\n",
    "\n",
    "#data_path = f'/home/laip/InternalScore/results/flask/{model_name}_with_feedback_logits.json'\n",
    "data_path = f'/home/laip/InternalScore/results/flask/{model_name}_logits.json'\n",
    "#data_path = '/home/laip/InternalScore/results/flask/Llama-2-7b-chat-hf_logits.json'\n",
    "#data_path = '/home/laip/InternalScore/results/flask/Mistral-7B-Instruct-v0___2_logits.json'\n",
    "all_human_score = []\n",
    "\n",
    "all_res = json.load(open(data_path))\n",
    "direct_score_ls = []\n",
    "weighted_score_ls = []\n",
    "weighted_direct_score_ls= []\n",
    "internalscore_ls = []\n",
    "save_ls = []\n",
    "pre_score_ls1 = []\n",
    "pre_score_ls2 = []\n",
    "pre_score_ls3 = []\n",
    "pre_score_ls4 = []\n",
    "ratio = 0.9\n",
    "cos = 0.02\n",
    "score = np.array([[1,2,3,4,5] for i in range(pd.DataFrame(all_res[0]['df']).shape[0])])\n",
    "final_score  = pd.DataFrame(score)\n",
    "\n",
    "\n",
    "for i in range(len(all_res)):\n",
    "    res = all_res[i]\n",
    "    \n",
    "    if res['weighted_socre'] == -1:\n",
    "        continue\n",
    "    all_human_score.append(res['human_score'])\n",
    "    df = pd.DataFrame(res['df']) \n",
    "    score = np.array([1,2,3,4,5])\n",
    "\n",
    "    # weighed_score 加权\n",
    "    distribution1 = df['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32).softmax(dim=-1))\n",
    "    pre_score1 = ((distribution1.apply(lambda x:(x*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum()))*weights).sum().item()\n",
    "\n",
    "    # 累积logits 加权\n",
    "    logits = df['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    distribution2 = (logits).apply(lambda x:torch.tensor(x,dtype=torch.float32).argmax(dim=-1))\n",
    "    pre_score = ((torch.tensor([ i+1 for i in distribution2]))*weights).sum().item()\n",
    "\n",
    "    distribution2 = torch.softmax((logits*weights).sum(),dim=-1)\n",
    "    pre_score2 = ((distribution2*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum()).item()\n",
    "    #print(pre_score2)\n",
    "    #pre_score2 = torch.argmax(distribution2).item()+1\n",
    "\n",
    "    # 累积logits 不加权    \n",
    "    distribution3 = torch.softmax((logits/weights.shape[0]).sum(),dim=-1)\n",
    "    pre_score3 = (distribution3*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum().item()\n",
    "\n",
    "    # ratio + cosine similarity 筛选\n",
    "   # logits = df[(df['ratio']>ratio)&(df['weights']>cos)]['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    logits = df[(df['ratio']>ratio)]['logits'].apply(lambda x:torch.tensor(x,dtype=torch.float32))\n",
    "    distribution4 = torch.softmax((logits/logits.shape[0]).sum(),dim=-1)\n",
    "    pre_score4 = (distribution4*torch.tensor([1,2,3,4,5],dtype=torch.float32)).sum().item()\n",
    "\n",
    "    prompt = res['prompt']\n",
    "    direct_score_ls.append(res['direct_socre'])\n",
    "    weighted_score_ls.append(res['weighted_socre'])\n",
    "    weighted_direct_score_ls.append(res['weighted_direct_socre'])\n",
    "    internalscore_ls.append(res['internalscore'])\n",
    "    pre_score_ls1.append(pre_score1)\n",
    "    pre_score_ls2.append(pre_score2)\n",
    "    pre_score_ls3.append(pre_score3)\n",
    "    pre_score_ls4.append(pre_score)\n",
    "    \n",
    "    #internalscore_ls.append((df['weighted_score']*weights).sum())\n",
    "#print(df[df['ratio']>r].shape[0])\n",
    "print(len(all_human_score),len(direct_score_ls),len(weighted_score_ls),len(weighted_direct_score_ls),len(internalscore_ls),len(pre_score_ls1))\n",
    "all_score_dict = {'direct_score':direct_score_ls,\n",
    "                  'weighted_score':weighted_score_ls,\n",
    "                  'weighted_direct_score':weighted_direct_score_ls,\n",
    "                  'internalscore':internalscore_ls,\n",
    "                  'pre_score1':pre_score_ls1,\n",
    "                  'pre_score2':pre_score_ls2,\n",
    "                  'pre_score3':pre_score_ls3,\n",
    "                  'pre_score4':pre_score_ls4\n",
    "                  }\n",
    "\n",
    "\n",
    "print_correlations(all_score_dict,all_human_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.064607928902617"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_abs(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return (np.abs(x-y)/len(x)).sum()\n",
    "cal_abs(pre_score_ls2,all_human_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_score_ls \n",
    "all_human_score\n",
    "pre_score_ls2\n",
    "pre_score_ls3\n",
    "data_name1 = 'direct_score'\n",
    "with open(f'{data_name1}.json','w') as f:\n",
    "    json.dump(direct_score_ls,f)\n",
    "data_name2 = 'human_score'\n",
    "with open(f'{data_name2}.json','w') as f:\n",
    "    json.dump(all_human_score,f)\n",
    "data_name3 = 'palmscore_w_tuning'\n",
    "with open(f'{data_name3}.json','w') as f:\n",
    "    json.dump(pre_score_ls3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "for i in range(1,7):\n",
    "    for j in range(1,7):\n",
    "        if i**2==4*j:\n",
    "            cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n",
    "\trow_len,col_len = len(a),len(a[0])\n",
    "\tb = [[0]*row_len]*col_len\n",
    "\tfor i in range(row_len):\n",
    "\t\tfor j in range(col_len):\n",
    "\t\t\tb[j][i] = a[i][j]\n",
    "\t\tprint(b)\n",
    "\treturn b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n",
    "\t#Write your code here and return a python list after reshaping by using numpy's tolist() method\n",
    "\trow_len,col_len = new_shape[0],new_shape[1]\n",
    "\tall_data = []\n",
    "\tfor i in range(len(a)):\n",
    "\t\tfor j in range(len(a[0])):\n",
    "\t\t\tall_data.append(a[i][j])\n",
    "\treshaped_matrix = [[0]*col_len for _ in range(row_len)]\n",
    "\tfor i in range(row_len):\n",
    "\t\tfor j in range(col_len):\n",
    "\t\t\treshaped_matrix[i][j] = all_data[i*col_len+j]\n",
    "\treturn reshaped_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3132616875182228"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
