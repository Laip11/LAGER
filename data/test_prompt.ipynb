{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"HelpSteer\"\n",
    "\n",
    "if data_name in ['flask', \"HelpSteer\"]:\n",
    "    df = pd.read_json(f\"data/spearman/{data_name}_preprocessed.json\", lines=True)\n",
    "    df.rename(columns={\"human_score\":\"score\"}, inplace=True)\n",
    "elif data_name==\"fb_bench\":\n",
    "    df = pd.read_parquet(\"data/1228/Feedback-Bench/data/train-00000-of-00001-eddf1add30d20be1.parquet\")\n",
    "    df.rename(columns={\"orig_score\":\"score\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_feedback = '''### Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given.\n",
    "- Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "- After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "\n",
    "### The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "### Response to evaluate:\n",
    "{output}\n",
    "\n",
    "### Score Rubrics:\n",
    "[{Definition}]\n",
    "- Score 1: {criteria_for_1}\n",
    "- Score 2: {criteria_for_2}\n",
    "- Score 3: {criteria_for_3}\n",
    "- Score 4: {criteria_for_4}\n",
    "- Score 5: {criteria_for_5}\n",
    "\n",
    "### Output format:\n",
    "The output format should look as follows: \\\"Feedback: <write your feedback here> Score: <give your score here>\\\"\n",
    "Remember that you should end with the the score. Please do not generate any other opening, closing, and explainations.'''\n",
    "\n",
    "template_no_feedback = '''### Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate and a score rubric representing a evaluation criteria are given.\n",
    "- Provide a single score that is an integer between 1 and 5 to assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "\n",
    "### The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "### Response to evaluate:\n",
    "{output}\n",
    "\n",
    "### Score Rubrics:\n",
    "[{Definition}]\n",
    "- Score 1: {criteria_for_1}\n",
    "- Score 2: {criteria_for_2}\n",
    "- Score 3: {criteria_for_3}\n",
    "- Score 4: {criteria_for_4}\n",
    "- Score 5: {criteria_for_5}\n",
    "\n",
    "### Output format:\n",
    "The output format should look as follows: \\\"score: <the score you give>\\\".\n",
    "Remember that you only need to provide the score. There is no need to generate explaination for it.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prompt(row, data_name, feedback=False):\n",
    "    template = template_feedback if feedback else template_no_feedback\n",
    "    if data_name in ['flask', \"HelpSteer\"]:\n",
    "        if data_name==\"flask\":\n",
    "            evaluation_aspect = row['criteria']\n",
    "            evaluation_criteria = row['criteria_dict'][evaluation_aspect]\n",
    "        elif data_name==\"HelpSteer\":\n",
    "            evaluation_aspect = row['criteria']\n",
    "            evaluation_criteria = eval(row['criteria_dict'])[evaluation_aspect]\n",
    "        res = template.format(\n",
    "            Definition=evaluation_criteria[\"definition\"],\n",
    "            criteria_for_1=evaluation_criteria[\"scoring_standards\"][\"1\"],\n",
    "            criteria_for_2=evaluation_criteria[\"scoring_standards\"][\"2\"],\n",
    "            criteria_for_3=evaluation_criteria[\"scoring_standards\"][\"3\"],\n",
    "            criteria_for_4=evaluation_criteria[\"scoring_standards\"][\"4\"],\n",
    "            criteria_for_5=evaluation_criteria[\"scoring_standards\"][\"5\"],\n",
    "            instruction=row['instruction'], \n",
    "            output=row['response']\n",
    "        )\n",
    "        return res\n",
    "    \n",
    "    elif data_name==\"fb_bench\":\n",
    "        res = template.format(\n",
    "            Definition=row[\"orig_criteria\"],\n",
    "            criteria_for_1=row[\"orig_score1_description\"],\n",
    "            criteria_for_2=row[\"orig_score2_description\"],\n",
    "            criteria_for_3=row[\"orig_score3_description\"],\n",
    "            criteria_for_4=row[\"orig_score4_description\"],\n",
    "            criteria_for_5=row[\"orig_score5_description\"],\n",
    "            instruction=row['orig_instruction'], \n",
    "            output=row['orig_response']\n",
    "        )\n",
    "        return res\n",
    "\n",
    "df['user_prompt'] = df.apply(lambda s: fill_prompt(s, data_name, feedback=False), axis=1)\n",
    "df['user_prompt_feedback'] = df.apply(lambda s: fill_prompt(s, data_name, feedback=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_prompt', \"user_prompt_feedback\", \"score\"]]\n",
    "df.to_json(f\"data/1228/{data_name}.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>user_prompt_feedback</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8940</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8941</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8942</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8943</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8944</th>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>### Task Description:\\nAn instruction (might i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8945 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            user_prompt  \\\n",
       "0     ### Task Description:\\nAn instruction (might i...   \n",
       "1     ### Task Description:\\nAn instruction (might i...   \n",
       "2     ### Task Description:\\nAn instruction (might i...   \n",
       "3     ### Task Description:\\nAn instruction (might i...   \n",
       "4     ### Task Description:\\nAn instruction (might i...   \n",
       "...                                                 ...   \n",
       "8940  ### Task Description:\\nAn instruction (might i...   \n",
       "8941  ### Task Description:\\nAn instruction (might i...   \n",
       "8942  ### Task Description:\\nAn instruction (might i...   \n",
       "8943  ### Task Description:\\nAn instruction (might i...   \n",
       "8944  ### Task Description:\\nAn instruction (might i...   \n",
       "\n",
       "                                   user_prompt_feedback  score  \n",
       "0     ### Task Description:\\nAn instruction (might i...      3  \n",
       "1     ### Task Description:\\nAn instruction (might i...      5  \n",
       "2     ### Task Description:\\nAn instruction (might i...      2  \n",
       "3     ### Task Description:\\nAn instruction (might i...      2  \n",
       "4     ### Task Description:\\nAn instruction (might i...      3  \n",
       "...                                                 ...    ...  \n",
       "8940  ### Task Description:\\nAn instruction (might i...      2  \n",
       "8941  ### Task Description:\\nAn instruction (might i...      4  \n",
       "8942  ### Task Description:\\nAn instruction (might i...      4  \n",
       "8943  ### Task Description:\\nAn instruction (might i...      2  \n",
       "8944  ### Task Description:\\nAn instruction (might i...      4  \n",
       "\n",
       "[8945 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "critic_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
